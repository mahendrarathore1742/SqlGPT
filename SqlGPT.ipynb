{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ✅ Install required libraries\n",
        "!pip install -q transformers peft datasets\n",
        "\n",
        "# ✅ Imports\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ✅ Load WikiSQL dataset\n",
        "dataset = load_dataset(\"wikisql\")\n",
        "\n",
        "# ✅ Use a small slice for testing (adjust sizes later!)\n",
        "train_dataset = dataset[\"train\"].select(range(5000))\n",
        "eval_dataset = dataset[\"validation\"].select(range(1000))\n",
        "\n",
        "# ✅ Load CodeT5-small\n",
        "model_name = \"Salesforce/codet5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# ✅ Apply LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ✅ Tokenization function (schema + question)\n",
        "def tokenize_wikisql(batch):\n",
        "    questions = batch[\"question\"]\n",
        "    sqls = [s[\"human_readable\"] for s in batch[\"sql\"]]\n",
        "    table_headers = [\" | \".join(t[\"header\"]) for t in batch[\"table\"]]\n",
        "    inputs_text = [\n",
        "        f\"question: {q} table: {t}\" for q, t in zip(questions, table_headers)\n",
        "    ]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        sqls,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# ✅ Tokenize datasets\n",
        "tokenized_train = train_dataset.map(tokenize_wikisql, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_eval = eval_dataset.map(tokenize_wikisql, batched=True, remove_columns=eval_dataset.column_names)\n",
        "\n",
        "# ✅ Training arguments (safe for Colab Free)\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./codet5-lora-sql\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=5e-4,\n",
        "    num_train_epochs=1,  # use 1 for testing\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    predict_with_generate=True,\n",
        "    fp16=True  # saves VRAM on Colab\n",
        ")\n",
        "\n",
        "# ✅ Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# ✅ Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# ✅ Train\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "SCbtaxaAQpqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = tokenizer(\n",
        "    \"question: How many students have a grade above 90? table: | name | grade |\",\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "output = model.generate(**test_input, max_length=64)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k90fMjZzS0Qp",
        "outputId": "e3d5f9d1-d630-4a3a-b818-25c90ba10d47"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT COUNT name FROM table WHERE grade = 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./SqlGPT\")\n",
        "tokenizer.save_pretrained(\"./SqlGPT\")\n"
      ],
      "metadata": {
        "id": "e6pwPTbEY3Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()  # paste your token\n",
        "\n",
        "model.push_to_hub(\"Mahendra1742/SqlGPT\")\n",
        "tokenizer.push_to_hub(\"Mahendra1742/SqlGPT\")\n"
      ],
      "metadata": {
        "id": "E-5Y5uPCZedj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Replace with your actual HF repo name\n",
        "model_name = \"Mahendra1742/SqlGPT\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Example input\n",
        "question = \"How many employees are in the Marketing department?\"\n",
        "table = \"| department | employees |\"\n",
        "\n",
        "prompt = f\"question: {question} table: {table}\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=128)\n",
        "\n",
        "print(\"OUTPUT :- \")\n",
        "print(\"   \")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "bFBt4weoZu5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xHcx6dkc5jM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}